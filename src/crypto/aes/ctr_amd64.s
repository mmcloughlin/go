#include "textflag.h"

DATA bswapMask<>+0x00(SB)/8, $0x08090a0b0c0d0e0f
DATA bswapMask<>+0x08(SB)/8, $0x0001020304050607
GLOBL bswapMask<>(SB), (NOPTR+RODATA), $16

// func encryptBlocks8Ctr(nr int, xk *uint32, dst, ctr *byte)
TEXT Â·encryptBlocks8Ctr(SB),0,$128-32
	MOVQ nr+0(FP), CX
	MOVQ xk+8(FP), AX
	MOVQ dst+16(FP), DX
	MOVQ ctr+24(FP), BX
	MOVQ 0(BX), R8
	MOVQ 8(BX), R9
	BSWAPQ R8
	BSWAPQ R9
	MOVOU bswapMask<>(SB), X9
	MOVQ R9, 0(SP)
	MOVQ R8, 8(SP)
	MOVOU 0(SP), X0
	PSHUFB X9, X0
	ADDQ $1, R9
	ADCQ $0, R8
	MOVQ R9, 16(SP)
	MOVQ R8, 24(SP)
	MOVOU 16(SP), X1
	PSHUFB X9, X1
	ADDQ $1, R9
	ADCQ $0, R8
	MOVQ R9, 32(SP)
	MOVQ R8, 40(SP)
	MOVOU 32(SP), X2
	PSHUFB X9, X2
	ADDQ $1, R9
	ADCQ $0, R8
	MOVQ R9, 48(SP)
	MOVQ R8, 56(SP)
	MOVOU 48(SP), X3
	PSHUFB X9, X3
	ADDQ $1, R9
	ADCQ $0, R8
	MOVQ R9, 64(SP)
	MOVQ R8, 72(SP)
	MOVOU 64(SP), X4
	PSHUFB X9, X4
	ADDQ $1, R9
	ADCQ $0, R8
	MOVQ R9, 80(SP)
	MOVQ R8, 88(SP)
	MOVOU 80(SP), X5
	PSHUFB X9, X5
	ADDQ $1, R9
	ADCQ $0, R8
	MOVQ R9, 96(SP)
	MOVQ R8, 104(SP)
	MOVOU 96(SP), X6
	PSHUFB X9, X6
	ADDQ $1, R9
	ADCQ $0, R8
	MOVQ R9, 112(SP)
	MOVQ R8, 120(SP)
	MOVOU 112(SP), X7
	PSHUFB X9, X7
	ADDQ $1, R9
	ADCQ $0, R8
	BSWAPQ R8
	BSWAPQ R9
	MOVQ R8, 0(BX)
	MOVQ R9, 8(BX)
	MOVUPS 0(AX), X8
	PXOR X8, X0
	PXOR X8, X1
	PXOR X8, X2
	PXOR X8, X3
	PXOR X8, X4
	PXOR X8, X5
	PXOR X8, X6
	PXOR X8, X7
	ADDQ $16, AX
	SUBQ $12, CX
	JE Lenc192
	JB Lenc128
Lenc256:
	MOVUPS 0(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 16(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	ADDQ $32, AX
Lenc192:
	MOVUPS 0(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 16(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	ADDQ $32, AX
Lenc128:
	MOVUPS 0(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 16(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 32(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 48(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 64(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 80(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 96(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 112(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 128(AX), X8
	AESENC X8, X0
	AESENC X8, X1
	AESENC X8, X2
	AESENC X8, X3
	AESENC X8, X4
	AESENC X8, X5
	AESENC X8, X6
	AESENC X8, X7
	MOVUPS 144(AX), X8
	AESENCLAST X8, X0
	AESENCLAST X8, X1
	AESENCLAST X8, X2
	AESENCLAST X8, X3
	AESENCLAST X8, X4
	AESENCLAST X8, X5
	AESENCLAST X8, X6
	AESENCLAST X8, X7
	MOVUPS X0, 0(DX)
	MOVUPS X1, 16(DX)
	MOVUPS X2, 32(DX)
	MOVUPS X3, 48(DX)
	MOVUPS X4, 64(DX)
	MOVUPS X5, 80(DX)
	MOVUPS X6, 96(DX)
	MOVUPS X7, 112(DX)
	RET

